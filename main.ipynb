{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mabzak/anime_subtitle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. get dataset dan convert ke dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Text Indonesia\n",
      "0                          Tidak, akulah yang salah.\n",
      "1                           Istriku tak selemah itu.\n",
      "2  Kira-kira apa yang terjadi di antara mereka be...\n",
      "3                    Nanti saja kupikirkan semuanya.\n",
      "4             Hanya itulah yang terpenting sekarang!\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "\n",
    "# Get kolom \"Text Indonesia\" dari train_data\n",
    "text_indonesia = [data['Text Indonesia'] for data in train_data]\n",
    "\n",
    "# Buat dataframe\n",
    "df = pd.DataFrame(text_indonesia, columns=[\"Text Indonesia\"])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi Stemmer Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "df['Text Indonesia'] = df['Text Indonesia'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah duplikat sebelum dihapus: 388\n"
     ]
    }
   ],
   "source": [
    "# jumlah duplikat sebelum dihapus\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Jumlah duplikat sebelum dihapus: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Text Indonesia  \\\n",
      "0                             tidak aku yang salah   \n",
      "1                              istri tak lemah itu   \n",
      "2       kira apa yang jadi di antara mereka dua ya   \n",
      "3                           nanti saja pikir semua   \n",
      "4                  hanya itu yang penting sekarang   \n",
      "...                                            ...   \n",
      "9834  apa pun yang jadi akan buat kau aku diri ini   \n",
      "9835      kita hanya perlu lewat uji tahap dua ini   \n",
      "9836        dan buka jalan untuk jadi orang hokage   \n",
      "9837              kau tak perlu ingat hal itu pada   \n",
      "9838                    alas untuk tak boleh kalah   \n",
      "\n",
      "                                   Text Lemmatized  \n",
      "0                             tidak aku yang salah  \n",
      "1                              istri tak lemah itu  \n",
      "2       kira apa yang jadi di antara mereka dua ya  \n",
      "3                           nanti saja pikir semua  \n",
      "4                  hanya itu yang penting sekarang  \n",
      "...                                            ...  \n",
      "9834  apa pun yang jadi akan buat kau aku diri ini  \n",
      "9835      kita hanya perlu lewat uji tahap dua ini  \n",
      "9836        dan buka jalan untuk jadi orang hokage  \n",
      "9837              kau tak perlu ingat hal itu pada  \n",
      "9838                    alas untuk tak boleh kalah  \n",
      "\n",
      "[9839 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menghapus duplikat\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Cetak data setelah menghapus duplikat\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Text Indonesia'])\n",
    "\n",
    "# Konversi teks menjadi sequence of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['Text Indonesia'])\n",
    "\n",
    "# Padding sequences\n",
    "maxlen = max([len(x) for x in sequences])\n",
    "X = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=maxlen))\n",
    "model.add(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0635 - loss: 116.7723 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0743 - loss: 1230.2529 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0619 - loss: 3425.5718 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0635 - loss: 6206.3218 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0502 - loss: 9440.4062 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0558 - loss: 13114.8086 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0510 - loss: 16446.2266 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0482 - loss: 20121.5977 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0466 - loss: 23754.5664 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0477 - loss: 27313.1445 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0441 - loss: 31378.7090 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0473 - loss: 35230.5859 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0465 - loss: 39285.2773 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0503 - loss: 42854.0156 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0504 - loss: 46848.6680 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0456 - loss: 51093.6328 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0441 - loss: 55259.4375 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0466 - loss: 59618.8906 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.0434 - loss: 62361.4297 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.0455 - loss: 68244.2734 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fb670d82d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tokenizer.texts_to_matrix(df['Text Indonesia'], mode='binary')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model dengan callback\n",
    "model.fit(X, y, epochs=20, batch_size=32, callbacks=[reduce_lr], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Jawab: itu\n"
     ]
    }
   ],
   "source": [
    "# Input dari pengguna\n",
    "test_input = input(\"Tanya: \")\n",
    "\n",
    "# Lemmatization\n",
    "test_input_lemmatized = stemmer.stem(test_input)\n",
    "\n",
    "# Tokenization\n",
    "test_input_seq = tokenizer.texts_to_sequences([test_input_lemmatized])\n",
    "test_input_pad = pad_sequences(test_input_seq, maxlen=maxlen)\n",
    "\n",
    "# Prediksi\n",
    "predicted = model.predict(test_input_pad)\n",
    "\n",
    "# Konversi hasil prediksi kembali ke teks\n",
    "predicted_text = tokenizer.sequences_to_texts([predicted.argmax(axis=1)])[0]\n",
    "\n",
    "# Output\n",
    "print(f\"Jawab: {predicted_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
